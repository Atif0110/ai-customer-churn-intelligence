# ğŸ“‰ AI Customer Churn Decision Intelligence Platform

An end-to-end **Decision Intelligence** system that combines **Data Science + Generative AI** to predict customer churn, explain risk drivers, and recommend retention actions.

This project demonstrates how **Machine Learning and LLMs can work together to support real business decisions â€” not just predictions.**

---

# ğŸš€ Overview

Customer churn is a major challenge for subscription and SaaS businesses.  
Predicting churn is useful â€” but **acting on it is what creates real value.**

This platform goes beyond prediction by providing:

- âœ… Churn probability scoring  
- âœ… Risk segmentation (Low / Medium / High)  
- âœ… Key driver identification  
- âœ… AI-generated retention advice  
- âœ… What-if scenario simulation  
- âœ… Full-stack deployment (API + UI)

It simulates a real **Decision Intelligence workflow** used in modern companies.

---

# ğŸ§  How It Works

## 1ï¸âƒ£ Data Science Layer

A **Logistic Regression model** predicts churn using:

- Monthly usage hours  
- Support tickets  
- Customer tenure  

### Outputs
- Churn probability  
- Risk level classification  

---

## 2ï¸âƒ£ Explainability Layer

Rule-based logic identifies key churn drivers:

- High support volume  
- Low product usage  
- Short tenure  

This makes predictions **interpretable for business users.**

---

## 3ï¸âƒ£ GenAI Layer

A Large Language Model (Groq / OpenAI / Gemini compatible) generates:

- Churn reasoning  
- Retention strategies  
- Business impact analysis  

This turns raw predictions into **actionable insights.**

---

## 4ï¸âƒ£ Decision Intelligence Layer

Scenario simulation allows users to test:

- "What if usage increases?"  
- "What if engagement improves?"  

This supports **proactive decision-making.**

---

# ğŸ— Architecture

Streamlit UI
â†“
FastAPI Backend
â†“
Orchestrator Service
â”œâ”€â”€ DS Model (scikit-learn)
â”œâ”€â”€ Explainability Logic
â””â”€â”€ LLM Provider Layer


- Modular design  
- Provider-agnostic LLM integration  
- Easy to extend and maintain  

### Supported LLM Providers

- Groq  
- OpenAI  
- Gemini  
- Local models  

---

# ğŸ“ Project Structure

backend/
â”‚
â”œâ”€â”€ api/ # FastAPI endpoints
â”œâ”€â”€ services/ # DS + GenAI logic
â”œâ”€â”€ llm/ # LLM provider abstraction
â”œâ”€â”€ ml/ # Model logic
â””â”€â”€ data/ # History storage

frontend/
â””â”€â”€ app.py # Streamlit UI

---

# âš™ï¸ Installation & Setup

## 1ï¸âƒ£ Create Virtual Environment

```bash
python -m venv venv
venv\Scripts\activate

2ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

3ï¸âƒ£ Configure Environment Variables

Create a .env file:

GROQ_API_KEY=your_key_here
LLM_PROVIDER=groq

â–¶ï¸ Running the Project
Start Backend
uvicorn backend.api.main:app --reload


Backend runs at:

http://127.0.0.1:8000

Start Frontend
streamlit run frontend/app.py


UI opens at:

http://localhost:8501